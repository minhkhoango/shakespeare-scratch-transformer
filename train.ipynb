{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c30d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import math\n",
    "from utils import *\n",
    "import importlib\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d8480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kagglehub\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86f618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: dimension of embeding vector output\n",
    "            num_heads: number of self attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model # 512\n",
    "        self.n_heads = n_heads # 8\n",
    "        self.d_k = d_model // n_heads # 512/8 = 64 : Each key, query, val will be of 64d\n",
    "\n",
    "        # key, query, and value matrixes # 64x64\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, _ = x.size() # batch, seq_len, d_model\n",
    "        return x.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_len, _ = x.size() # batch_size, n_heads, seq_len, d_k\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, Q, K, V, mask=None): # batch_size x sequence_length x embedding_dim # 32x10x512\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attn_scores = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_scores))\n",
    "        return output\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_off):\n",
    "        \"\"\"\n",
    "        d_model: embedding_size, model's width\n",
    "        d_ff: feed forward dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_off)\n",
    "        self.fc2 = nn.Linear(d_off, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# class EncoderLayer(nn.Module):\n",
    "#     def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "#         self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "#         self.norm1 = nn.LayerNorm(d_model)\n",
    "#         self.norm2 = nn.LayerNorm(d_model)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "#     def forward(self, x, mask):\n",
    "#         attn_output = self.self_attn(x, x, x, mask)\n",
    "#         x = self.norm1(x + self.dropout(attn_output))\n",
    "#         ff_output = self.feed_forward(x)\n",
    "#         x = self.norm2(x + self.dropout(ff_output))\n",
    "#         return x\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "   def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "      super().__init__()\n",
    "\n",
    "      self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "      self.cross_attn = MultiHeadAttention(d_model, n_heads)\n",
    "      self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "      \n",
    "      self.norm1 = nn.LayerNorm(d_model)\n",
    "      self.norm2 = nn.LayerNorm(d_model)\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "   def forward(self, x, tgt_mask):\n",
    "      # casual self-attention\n",
    "      attn = self.self_attn(x, x, x, tgt_mask)\n",
    "      x = self.norm1(x + self.dropout(attn))\n",
    "\n",
    "      ff_output = self.feed_forward(x)\n",
    "      x = self.norm2(x + self.dropout(ff_output))\n",
    "      return x\n",
    "   \n",
    "class GPTLike(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, n_heads=8, \n",
    "                 n_layers=6, d_ff=512, max_seq_len=256, dropout=0.1):\n",
    "        super().__init__() \n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.out_proj = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def causual_mask(sz, device):\n",
    "        return torch.tril(torch.ones(sz, sz, device=device)).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "\n",
    "        tok = self.token_emb(x)\n",
    "        h = self.pos_enc(tok)\n",
    "\n",
    "        mask = GPTLike.causual_mask(T, x.device)\n",
    "\n",
    "        for blk in self.layers:\n",
    "            h = blk(h, mask)\n",
    "\n",
    "        h = self.ln_final(h)\n",
    "        logits = self.out_proj(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e9d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word embeddings pretrained\n",
    "data_path = 'data.txt'\n",
    "text = load_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc918c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input and build vocab\n",
    "tokens, word2idx, idx2word = tokenize_and_build_vocab(text)\n",
    "\n",
    "pad_idx  = word2idx['<pad>']\n",
    "unk_idx  = word2idx['<unk>']\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4b61a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTLike(\n",
       "  (token_emb): Embedding(30992, 128)\n",
       "  (pos_enc): PositionalEncoding()\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x DecoderBlock(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_proj): Linear(in_features=128, out_features=30992, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, val_loader = train_eval_split(tokens, word2idx, batch_size=1300)\n",
    "\n",
    "transformer = GPTLike(vocab_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=3e-4, betas=(0.9, 0.95))\n",
    "\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21880ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    loss = train_one_epoch(transformer, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ecc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(), \"transformer_state.pth\")\n",
    "config = {\n",
    "    \"vocab_size\": len(word2idx),\n",
    "    \"d_model\": 128,\n",
    "    \"n_heads\": 8,\n",
    "    \"n_layers\": 6,\n",
    "    \"d_ff\": 512,\n",
    "    \"max_seq_len\": 256,\n",
    "    \"dropout\": 0.1\n",
    "}\n",
    "with open(\"config.pkl\", \"wb\") as f:\n",
    "    pickle.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd94a977-429d-496c-a07f-a3b1bf284205",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.pkl\", \"rb\") as f:\n",
    "    config = pickle.load(f)\n",
    "\n",
    "transformer = GPTLike(**config)\n",
    "transformer.load_state_dict(torch.load(\"transformer_state.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d67aba-2b33-47a5-9da2-8dcd6a4f75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"thou art\"\n",
    "prompt_ids, prompt_tokens = encode_prompt(prompt, word2idx)\n",
    "\n",
    "print(prompt_ids)\n",
    "print([idx2word[i] for i in prompt_ids.squeeze(0).tolist()])\n",
    "\n",
    "out_ids = generate(transformer, prompt_ids, 100)\n",
    "print(' '.join(idx2word[i] for i in out_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa48904-bc13-4593-8784-8f7005567bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
